{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Sentiment Analysis 1"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Idenitifying sentiment in large quantities of textual data is a popular Natural Language Processing application. Being able to recognize patterns of feeling in user-submitted reviews with a standardized approach to interpretation allows users to fastly and consistently evaluate unseen text. While errors in interpretation certainly occur, sentiment analysis has proven to be a powerful, well-documented tool in business, hospitality, and even some humanities work.\n\nCommon forms of figurative language like irony, metaphor, understatement, and overstatement should make us duly skeptical about sentiment analysis's accuracy. Still, the components of sentiment analysis provide a strong primer to classification and machine learning tasks. \n\nThis tutorial will focus on the **annotation** and **classification** components of sentiment analysis. By its end, you should feel comfortable with creating an annotated dataset and using it to classify new text. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Annotation\n\nTo track positive and negative sentiment in a text, you first need to establish which feature words tend to have positive and negative assosciations. Granted, this is not as easy as it sounds. Take the following two sentences for example: 1.) I *killed* my prelim. 2.) I *killed* your pet goldfish. While the verb *killed* carries a positive, albeit more colloqiual connotation, in example one, it's deeply upsetting in example two. By that token, performing sentiment analysis on the journal of a succesful, informal businessman who kills all of life's challenges would be different from doing the same on a police report on John Wayne Gacy.\n\n**Annotation** formalizes the practices and prodedures for labeling features in textual data. Choosing textual data that is widely accessible and represents a somewhat standard, undecorated use of language is typically favored. Unless your project specifically requires that you engage with the language of a particular time-period, mindset, or type of textual material (i.e. police reports or prelim acers), it's better to choose widely disemminated non-fictional prose. Newspapers and journals are popular.\n\n**Annotation Guidelines** are an established set of rules meant to prepare and train a number of additional annotators to a datset. For example, imagine that Marcus likes to read the end of novels after starting them, whereas Stephanie reads sequentially, but will reread chapters she that make her happy before proceeding. If both Marcus and Stephanie are given the task to annotate positive and negative sentiment in Thomas Hardy's *Jude the Obscure*, it's highly likely that their reading practices would inflect how they annotated lines in the text; the former might read nothing in the text as positive, whereas the latter might overemphasize the positive. Annotation Guidelines are intended to inform and make uniform several annotators knowledge and approach to labeling data.\n\n**Inter-annotator Agreement** gauges how similar the results of several annotators are when labeling a particular dataset. While there's not a particular metric for how high inter-annotator agreement should be (it depends a lot on the dataset and the task), having low inter-annotator agreement may be a sign that either the annotation task is too difficult or the guidelines are too vague.\n\nFor this tutorial, we're going to annotate verbs that connote *surprise* against words that connote *knowing*."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Inter-annotator Agreement\n\n1.) Look at each verb. If you are unsure of its meaning or connotations, please refer to the Oxford English Dictionary.\n2.) If a verb connotes **surprise**, place a **1** next to the comma. If a word connotes **knowing**, place a **-1** next to the comma.\n3.) If a word seems to fit neither category place a **0** next to it. Additionally, if you find a word to only sort of imply surprise or knowing, place a **.5** or **-.5** next to it, respectively.\n4.) Once finished, run the window and then use the *check_annotations* function to guarantee that they have been succesfully input."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "surprise_and_knowing_user = [(\"intuit\",),(\"apprehend\",),(\"shock\",),(\"amaze\",),(\"confuse\",)\n                        ,(\"thrill\",),(\"feel\",),(\"instill\",),(\"jolt\",),(\"startle\",)\n                             ,(\"amuse\",),(\"affect\",),(\"think\",),(\"understand\",),(\"command\",)\n                        ,(\"horrify\",),(\"recognize\",),(\"notice\",),(\"mesmerize\",),(\"graph\",)\n                       ,(\"intend\",),(\"value\",),(\"remember\",),(\"avow\",),(\"astonish\",)\n                        ,(\"cherish\",),(\"accept\",),(\"recollect\",),(\"happen\",),(\"meditate\",)\n                       ,(\"recoil\",),(\"animate\",),(\"remind\",),(\"master\",),(\"cower\",)\n                        ,(\"rehearse\",),(\"compell\",),(\"preside\",),(\"present\",),(\"terrify\",)\n                       ,(\"elate\",),(\"harbor\",),(\"corner\",),(\"excite\",),(\"relish\",)\n                        ,(\"tender\",),(\"alarm\",),(\"browse\",),(\"archive\",),(\"anticipate\",)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "surprise_and_knowing_Malcolm = [(\"intuit\",-1),(\"apprehend\",-1),(\"shock\",1),(\"amaze\",1),(\"confuse\",.5)\n                        ,(\"thrill\",1),(\"feel\",-.5),(\"instill\",-1),(\"jolt\",1),(\"startle\",1)\n                       ,(\"amuse\",.5),(\"affect\",.5),(\"think\",-.5),(\"understand\",-1),(\"command\",-.5)\n                        ,(\"horrify\",1),(\"recognize\",-1),(\"notice\",0),(\"mesmerize\",.5),(\"graph\",-.5)\n                       ,(\"intend\",-.5),(\"value\",-1),(\"remember\",-1),(\"avow\",-.5),(\"astonish\",1)\n                        ,(\"cherish\",-1),(\"accept\",0),(\"recollect\",-1),(\"happen\",1),(\"meditate\",-.5)\n                       ,(\"recoil\",1),(\"animate\",0),(\"remind\",-1),(\"master\",-1),(\"cower\",1)\n                        ,(\"rehearse\",-1),(\"compell\",.5),(\"preside\",-.5),(\"present\",.5),(\"terrify\",.5)\n                       ,(\"elate\",1),(\"harbor\",-.5),(\"corner\",1),(\"excite\",1),(\"relish\",-.5)\n                        ,(\"tender\",-.5),(\"alarm\",1),(\"browse\",.5),(\"archive\",-.5),(\"anticipate\",-1)]\n\n#I annotated about five minutes after making this list of words. In theory, I shouldn't be the one annotating the list of words I create#",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "surprise_and_knowing_perturbed = [(\"intuit\",-.5),(\"apprehend\",-.5),(\"shock\",-1),(\"amaze\",0),(\"confuse\",-1)\n                        ,(\"thrill\",1),(\"feel\",-.5),(\"instill\",-1),(\"jolt\",1),(\"startle\",1)\n                       ,(\"amuse\",.5),(\"affect\",.5),(\"think\",-.5),(\"understand\",-1),(\"command\",-.5)\n                        ,(\"horrify\",1),(\"recognize\",-1),(\"notice\",0),(\"mesmerize\",.5),(\"graph\",-.5)\n                       ,(\"intend\",-.5),(\"value\",-1),(\"remember\",-1),(\"avow\",-.5),(\"astonish\",1)\n                        ,(\"cherish\",-1),(\"accept\",0),(\"recollect\",-1),(\"happen\",1),(\"meditate\",-.5)\n                       ,(\"recoil\",1),(\"animate\",0),(\"remind\",-1),(\"master\",-1),(\"cower\",1)\n                        ,(\"rehearse\",-1),(\"compell\",.5),(\"preside\",-.5),(\"present\",.5),(\"terrify\",.5)\n                       ,(\"elate\",1),(\"harbor\",-.5),(\"corner\",1),(\"excite\",1),(\"relish\",-.5)\n                        ,(\"tender\",-.5),(\"alarm\",1),(\"browse\",.5),(\"archive\",-.5),(\"anticipate\",-1)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def check_annotations(list):\n    for item in list:\n        if item[1] in [-1,-.5,0,.5,1]:\n            pass     \n        else:\n            print(item[0] + \" has an incorrect or missing annotation.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#if you receive a \"tuple index out of range\" error when running this, check to make sure that all of the tuples above \n#are assigned a value after the comma#\n\ncheck_annotations(surprise_and_knowing_user)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Scoring System\n\nTo measure interannotator agreement, we need to create a **scoring system** capable of measuring the similarity in our annotations. Since we are creating a dataset for a **binary classifier**, we may not want to penalize disagreements on the degree of classification (\"terrify\" being a .5 vs. a 1) as much as we would an absolute difference of classification (\"affect\" being a -.5 or a 5). The system below will provide 2 points for all identical annotations (\"astonish\" being a 1 for each) and 1 point for each annotation that's the same sign. If the disagreement is larger than that, no points will be awarded. It will return a list of instances where disagreement occured."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def agreement_scorer(list1, list2):\n    score = 0\n    disagreement = []\n    for entry in range(len(list1)):\n        if list1[entry][1] == list2[entry][1]:\n            score += 2\n        elif (list1[entry][1] + list2[entry][1])/2 in [0.75, -0.75]:\n            score += 1\n        else:\n            print(\"On entry \" + str(entry) + \" we disagree:\")\n            print(list1[entry], list2[entry])\n    return(score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#this is a test to show you what disagreement might look like#\nagreement_scorer(surprise_and_knowing_Malcolm, surprise_and_knowing_perturbed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#test our disagreement here#\nagreement_scorer(surprise_and_knowing_Malcolm, surprise_and_knowing_user)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Once we've established the degree of disagreement in our annotations, we have a two options for how to proceed: 1.) we can force a compromise and average our scores. 2.) we can eliminate all of the entries we disagreed on.\n\nThe first option is beneficial because we do not lose any training data. Since we're dealing with only fifty annotated verbs, every entry counts. By the same token, however, it's important to look at the three disagreements in the perturbed example. Were we to average results, \"shock\" would be labeled as a 0 in a training set meant to classify surprise. Equally as unsettling, the second annotator's decision to label \"confuse\" as being absolutely representative of \"knowing\" outweighs the first annotator's more moderate decision to label the term as representative of surprise. We would, for all intents and purposes, be introducing bad data into our training set.\n\nThe second option is beneficial because it relies solely on a consensus between annotators. Issues with the classifier's results can be traced back to the annotation directions and dataset rather than the peculiarities of the annotator. Granted, this approach also ignores various levels of fluency and expertise in annotators. A native speaker whose dissertation is on textual representations of surprise could have their annotations discounted because they disagreed with a mechanical turker who performed the annotations while watching television.\n\nWhile there is not a correct answer, we can prepare both approaches below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def force_compromise (list1, list2):\n    agreement = []\n    for entry in range(len(list1)):\n        if list1[entry][1] == list2[entry][1]:\n            agreement.append(list1[entry])\n        else:\n            agreement.append((list1[entry][0], (list1[entry][1] + list2[entry][1])/2))\n    return(agreement)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#replace perturbed with user#\nfc_List = force_compromise(surprise_and_knowing_Malcolm, surprise_and_knowing_perturbed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def no_compromise(list1, list2):\n    agreement = []\n    for entry in range(len(list1)):\n        if list1[entry][1] == list2[entry][1]:\n            agreement.append(list1[entry])\n    return(agreement)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#replace perturbed with user#\nnc_List = no_compromise(surprise_and_knowing_Malcolm, surprise_and_knowing_perturbed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nc_List",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def some_compromise(list1, list2):\n    agreement = []\n    for entry in range(len(list1)):\n        if list1[entry][1] == list2[entry][1]:\n            agreement.append(list1[entry])\n        elif (list1[entry][1] + list2[entry][1])/2 in [0.75, -0.75]:\n            agreement.append((list1[entry][0], (list1[entry][1] + list2[entry][1])/2))\n        else:\n            pass\n    return(agreement)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#replace perturbed with user#\nsc_List = some_compromise(surprise_and_knowing_Malcolm, surprise_and_knowing_perturbed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(fc_List)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(sc_List)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(nc_List)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fc_List[\"intuit\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now have annotated and synthesized the data necessary for a small, specialized sentiment classifier! Before using any kind of sentiment analysis tool, it is important to look at the dataset, annotation procedures, and documentation. You may discover that the intention or process of a particular datset does not align with your project. For instance, if you were focused on fifteenth-century theatre, you may find that a dataset made for testing movie reviews from the late twentieth century maybe be inappropriate."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Classification\n\nNow that we've generalized about which verbs generally imply surprise and knowing, we can try using this data to classify surprise and knowing in new sentences. Knowing what you know about our dataset, how do you expect the following sentences to be classified?\n\n1.) She thought the pick-up line had been rehearsed.\n2.) Sally was alarmed by the quantity of parpika on the flounder.\n3.) I was shocked by how well I understood my way around Dublin.\n\nThe presence of annotated terms and the lack of figurative language in sentences 1 and 2 make them pretty easy to classify. Sentence 3, however, poses some difficulties; the speaker is surprised by how much they knew.\n\nOne way to resolve this ambiguity is to produce a line in the sand and stick to it. One might argue that, since \"understood\" is modified by \"well\" in sentence 3 while \"shocked\" has no modifier, understanding is the empasis of the sentence. They might then make a rule that the prevalance of verb modifiers should make the call. Another might argue that since \"shocked\" comes first in the sentence, it is more important than \"understood\" and that a rule should be made to favor verbs that come earlier in sentences."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_data = [\"She thinks the pick-up line had been rehearsed.\", \"Sally was alarmed by the quantity of parpika on the flounder.\"\n            , \"It is astonishing how well I understand my way around Dublin.\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To make a rudimentary classifier, we might consider starting by making another scoring system. If a word from our annotations appears, we'll assign it the value from our annotations."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def score_sentence(annotations, test_data_sentence):\n    score = 0\n    terms = []\n    for item in annotations:\n        if item[0] in test_data_sentence:\n            score += item[1]\n            terms.append(item)\n    print(test_data_sentence)\n    print(terms)\n    print(score)\n    if score != 0:\n        print(score/len(terms))\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for line in test_data:\n    score_sentence(fc_List,line)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try this with a few more sentences to determine how well it's working."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_data_expanded = [\"This was not what I expected at all\", \"I had a feeling you might say that\", \"Winston's values were clear: whole wheat and pastries did not mix\",\n                     \"The scary movie did not terrify the child.\", \"The command strips happened to be amazingly costly\", \"Claire browsed the chainsaws, unsure of her next move.\",\n                     \"While I do not harbor grudges, I do harbor sailboats for a small fee.\", \"The slogan suggested the reader cherish the present\", \n                     \"She was mesmerized by the neon sign\", \"Hector felt a jolt of excitment when she realized that the thrill was gone and she would shoplift no longer\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for line in test_data_expanded:\n    score_sentence(fc_List,line)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As you can see, there are some issues with the scores we have. The most glaring is that the context of the sentence is being ignored in favor of the appearance of a key word; \"I am not surprised\" would be classified as an instance of surprise, despite the presence of the \"not.\" Other issues involve the fact that some verbs can also be used as nouns that have relatively different meanings. In the \"cherish the present\" sentence, \"present\" isn't a term that has much to do with knowing or surprise; in a sentence that read \"Cynthia was presented with flours after her dance recital,\" it might have more to do with surprise.\n\nIn the second part of this tutorial, we will look at **dependency parsing**, namely **Part-of-Speech Tagging**. This, combined with larger dictionaries and significantly more annotations will allow us to start working around some of these issues. "
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}